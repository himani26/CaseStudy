{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMN2VsKdCL5f86q15g3wHr3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himani26/CaseStudy/blob/main/Trinity_fine_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trinity ChatQnA**:\n"
      ],
      "metadata": {
        "id": "0ImBPl4ZIEO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installations**"
      ],
      "metadata": {
        "id": "guRSS4vZhide"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q transformers peft datasets bitsandbytes accelerate sentencepiece pypdf2 streamlit pyngrok anthropic"
      ],
      "metadata": {
        "id": "OqZviL0RIL75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "id": "zCkMUtZvMgZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, torch, textwrap, json, openai\n",
        "from PyPDF2 import PdfReader\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel"
      ],
      "metadata": {
        "id": "lpgsWQ6UIPda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3Ô∏è‚É£ OpenAI API Key (for Q&A generation)\n",
        "from anthropic import Anthropic\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"secret_key_here\"\n",
        "\n",
        "client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))"
      ],
      "metadata": {
        "id": "JNg7n4uBIY5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Upload files for training**"
      ],
      "metadata": {
        "id": "5MicmGZbhyhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "7kdA0q1cI1OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_text_from_file(path):\n",
        "    if path.lower().endswith(\".pdf\"):\n",
        "        reader = PdfReader(path)\n",
        "        return \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
        "    elif path.lower().endswith(\".txt\"):\n",
        "        return open(path).read()\n",
        "    else:\n",
        "        raise ValueError(\"Only PDF or TXT supported\")\n",
        "\n",
        "doc_text = extract_text_from_file(file_name)\n",
        "print(\"‚úÖ Loaded document:\", file_name, f\"({len(doc_text)} characters)\")"
      ],
      "metadata": {
        "id": "vbZVyn8oLgpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating QnA Pairs**\n",
        "For each chunk generating 8 chunks for training purpose"
      ],
      "metadata": {
        "id": "uLPzASSLh5FE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_qna_pairs(text, num_pairs=8):\n",
        "    import re, json\n",
        "    chunks = textwrap.wrap(text, 1500)\n",
        "    qa_pairs = []\n",
        "\n",
        "\n",
        "    for chunk in chunks[:3]:\n",
        "        prompt = f\"\"\"\n",
        "        Generate {num_pairs} factual question‚Äìanswer pairs from the text below.\n",
        "        Return ONLY valid JSON array (no markdown, no explanations).\n",
        "\n",
        "        Text:\n",
        "        {chunk}\n",
        "        \"\"\"\n",
        "        response = client.messages.create(\n",
        "            model=\"claude-3-opus-20240229\",\n",
        "            messages=[\n",
        "\n",
        "              {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "\n",
        "        # content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        content = response.content[0].text.strip()\n",
        "        content = re.sub(r\"^```json|```$\", \"\", content.strip())\n",
        "        content = re.sub(r\"^```|```$\", \"\", content.strip())\n",
        "\n",
        "        match = re.search(r\"\\[.*\\]\", content, re.S)\n",
        "        if match:\n",
        "            json_str = match.group(0)\n",
        "            try:\n",
        "                pairs = json.loads(json_str)\n",
        "                qa_pairs.extend(pairs)\n",
        "            except Exception as e:\n",
        "                print(\"‚ö†Ô∏è JSON decode error:\", e)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No valid JSON block detected.\")\n",
        "\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "nz0vkIDPLnfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lis_qna = generate_qna_pairs(doc_text)"
      ],
      "metadata": {
        "id": "TmGOUmeTSb07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lis_qna)"
      ],
      "metadata": {
        "id": "SevX_1I8SzKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizing the generated QnA Pairs**"
      ],
      "metadata": {
        "id": "FUenH8sciIGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for item in lis_qna:\n",
        "    if \"question\" in item:\n",
        "        item[\"instruction\"] = item.pop(\"question\")\n",
        "    if \"answer\" in item:\n",
        "        item[\"response\"] = item.pop(\"answer\")\n",
        "dataset = Dataset.from_list(lis_qna)\n",
        "base_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "def format_and_tokenize(example):\n",
        "    prompt = f\"<|user|> {example['instruction']}\\n<|assistant|> {example['response']}\"\n",
        "    toks = tokenizer(prompt, truncation=True, max_length=512, padding=\"max_length\")\n",
        "    toks[\"labels\"] = toks[\"input_ids\"].copy()\n",
        "    return toks\n",
        "\n",
        "tokenized_ds = dataset.map(format_and_tokenize)\n",
        "print(\"‚úÖ Tokenized\", len(tokenized_ds), \"examples\")"
      ],
      "metadata": {
        "id": "clm_PfjXLyAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine Tuning with LORA**"
      ],
      "metadata": {
        "id": "jQbIMcZ8iTT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model, load_in_8bit=True, device_map=\"auto\")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\",\"v_proj\"],\n",
        "                         lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    output_dir=\"./lora-finetune\",\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, train_dataset=tokenized_ds, args=training_args)\n",
        "print(\"üöÄ Fine-tuning started...\")\n",
        "trainer.train()\n",
        "model.save_pretrained(\"lora-finetune\")\n",
        "tokenizer.save_pretrained(\"lora-finetune\")\n",
        "print(\"‚úÖ LoRA adapter saved in ./lora-finetune\")"
      ],
      "metadata": {
        "id": "ELZ-3DIiMIaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile streamlit_lora_app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "st.set_page_config(page_title=\"LoRA QA Chatbot\", layout=\"wide\")\n",
        "st.title(\"üí¨ LoRA Fine-Tuned Chatbot\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    base = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", load_in_8bit=True, device_map=\"auto\")\n",
        "    model = PeftModel.from_pretrained(base, \"lora-finetune\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model()\n",
        "\n",
        "def generate_answer(prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(**input_ids, max_new_tokens=512, temperature=0.7,eos_token_id=None)\n",
        "    txt = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    # Remove role tokens\n",
        "    txt = txt.replace(\"<|user|>\", \"\").replace(\"<|assistant|>\", \"\").strip()\n",
        "    return txt.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "if \"chat\" not in st.session_state:\n",
        "    st.session_state.chat = []\n",
        "\n",
        "user_input = st.text_input(\"Ask something related to the uploaded document:\")\n",
        "if st.button(\"Send\") and user_input:\n",
        "    prompt = f\"<|user|> {user_input}\\n<|assistant|>\"\n",
        "    answer = generate_answer(prompt)\n",
        "    st.session_state.chat.append((\"üßë\", user_input))\n",
        "    st.session_state.chat.append((\"ü§ñ\", answer))\n",
        "\n",
        "for s, m in reversed(st.session_state.chat):\n",
        "    if s == \"üßë\": st.markdown(f\"**{s} You:** {m}\")\n",
        "    else: st.info(f\"**{s} Bot:** {m}\")"
      ],
      "metadata": {
        "id": "OgDIdP3XTmTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"secret_key_here\")\n",
        "print(\"üöÄ Launching chatbot...\")\n",
        "get_ipython().system_raw(\"streamlit run streamlit_lora_app.py --server.port 8501 &\")\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(\"‚úÖ Your chatbot is live at:\", public_url)"
      ],
      "metadata": {
        "id": "rtUx1pZgULo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qp8c3fd5cC64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "08A6yUqC-ULd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cG0aCSHVUjNC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}