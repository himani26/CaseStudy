
# ğŸ§  Domain-Tuned LLM with LoRA (TinyLlama)

Fine-tuning a lightweight open-source LLM on your documents for domain-specific Q&A

This project demonstrates how to fine-tune a TinyLlama 1.1B Chat model using LoRA (Low-Rank Adaptation) on your own PDF/TXT documents to build a private, domain-specialized AI assistant.

âœ… Upload any domain document

âœ… Auto-extract text & generate Q&A pairs

âœ… Fine-tune TinyLlama efficiently

âœ… Deploy a chat UI using Streamlit

âœ… Works locally / on Colab

âœ… No cloud vendor lock-in

â¸»

ğŸš€ Features

Capability	Description
ğŸ“„ Document ingestion	Upload PDFs/TXT for training
ğŸ¤– Synthetic Q&A generation	LLM auto-creates training pairs
âš¡ Efficient LoRA training	1B model, k-bit + PEFT
ğŸ§ª Domain-aware responses	Answers only from your document
ğŸ–¥ï¸ Chat interface	Streamlit chatbot included
ğŸ” Private & local	No data leaves your environment


â¸»

ğŸ“¦ Tech Stack
	â€¢	ğŸ¤— Transformers
	â€¢	ğŸ¦™ TinyLlama 1.1B Chat
	â€¢	ğŸ§  LoRA + PEFT
	â€¢	âš™ï¸ 8-bit quantization (k-bit training)
	â€¢	ğŸ Python
	â€¢	ğŸ¨ Streamlit UI

â¸»

ğŸ’¡ Why LoRA + TinyLlama?

Benefit	Reason
ğŸš€ Fast	Train in under an hour on Colab T4/A100
ğŸ’¾ Small	Only ~1B params â€” deployable anywhere
ğŸ’° Cheap	No high-end hardware required
ğŸ”’ Private	Fine-tune locally, no data sharing
ğŸ“ Flexible	Drop-in replacement for SaaS models


â¸»

ğŸ› ï¸ Training Workflow

1ï¸âƒ£ Upload a document

Reads PDF/TXT and extracts text.

2ï¸âƒ£ Auto-generate Q&A dataset

Uses LLM to create domain-grounded questions.

3ï¸âƒ£ Format & tokenize prompts

Uses TinyLlama chat format:

<|user|> question
<|assistant|> answer

4ï¸âƒ£ Fine-tune using LoRA

Low-rank adapters â†’ fast + lightweight.

5ï¸âƒ£ Chat Interface

Real-time Q&A from your tuned model.

â¸»

â–¶ï¸ Run the Chatbot

streamlit run streamlit_lora_app.py

Then open the browser URL to chat with your fine-tuned model.

â¸»

ğŸ“Š Evaluation Approach
	â€¢	Manual benchmark on held-out questions
	â€¢	Context-fidelity evaluation
	â€¢	Hallucination check
	â€¢	Qualitative response accuracy

â¸»

ğŸ”¥ Results

âœ”ï¸ Strong domain recall
âœ”ï¸ Improved factual accuracy
âœ”ï¸ Very low hallucination rate (after format fix)
âœ”ï¸ Fast inference on GPU / Colab




